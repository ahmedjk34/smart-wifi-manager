"""
Intermediate ML Data Cleaning and Statistical Analysis Pipeline
Processes smart-v3-logged-ALL.csv with advanced cleaning, filtering, and comprehensive statistics.

CRITICAL FIXES (2025-10-02 14:26:45 UTC):
- Issue C6: Updated imbalance thresholds to match File 1b output (20x from POWER=0.5)
- Issue M5: Corrected validation expectations (File 1b DOES balance, this is correct)
- Issue C3: NOW REMOVES OUTCOME FEATURES (shortSuccRatio, packetLossRate, etc.)

WHAT WAS WRONG BEFORE:
‚ùå File 2 expected 5-20x imbalance (natural WiFi)
‚ùå But File 1b creates 20x imbalance (POWER=0.5 balancing)
‚ùå This caused FALSE WARNING: "Distribution looks artificially balanced!"
‚ùå Outcome features (shortSuccRatio, packetLossRate) were NOT removed in File 2

WHAT'S FIXED NOW:
‚úÖ File 2 expects 15-30x imbalance (matches File 1b POWER=0.5 output)
‚úÖ No false warnings about "artificial balancing"
‚úÖ Clear documentation: File 1b DOES balance (this is CORRECT behavior)
‚úÖ File 2 only does statistical cleaning (no class balancing)
‚úÖ REMOVES OUTCOME FEATURES (5 features: shortSuccRatio, medSuccRatio, packetLossRate, severity, confidence)

CRITICAL: This file does NOT balance classes - that happens in File 1b (CSV combiner)
          Balancing here would destroy real WiFi traffic characteristics!

Features:
- Advanced duplicate detection and removal
- Extensive outlier detection and filtering
- Data type enforcement and validation
- Comprehensive statistical analysis and visualization
- NO CLASS BALANCING (maintains File 1b distribution)
- Detailed logging and reporting
- Export cleaned data and statistics
- FIXED: Early removal of constant/useless features (Issue #28)
- FIXED: Reproducible random seed (Issue #14)
- FIXED: Correct imbalance expectations (Issue C6)
- FIXED: Removes outcome features (Issue C3)

Author: ahmedjk34
Date: 2025-10-02 14:26:45 UTC (FULLY FIXED)
FIXED: Issues #14, #28, C3, C6, M5
"""

import warnings
import sys
import logging

# CRITICAL: Suppress ALL warnings BEFORE any other imports
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')

# Redirect stderr to devnull to suppress C-level warnings
import os
sys.stderr = open(os.devnull, 'w')

# NOW import pandas and numpy
import pandas as pd
import numpy as np

# Suppress pandas warnings
pd.options.mode.chained_assignment = None
pd.set_option('mode.copy_on_write', True)

# Suppress numpy warnings
np.seterr(all='ignore')

import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
from scipy import stats
import json

# ================== CONFIGURATION ==================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(BASE_DIR)
INPUT_CSV = os.path.join(PARENT_DIR, "smart-v3-logged-BALANCED.csv")
OUTPUT_CSV = os.path.join(PARENT_DIR, "smart-v3-ml-cleaned.csv")
STATS_DIR = os.path.join(BASE_DIR, "cleaning_stats")
LOG_FILE = os.path.join(BASE_DIR, "intermediate_cleaning.log")

# FIXED: Issue #14 - Set random seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# WiFi-specific constraints
VALID_RATE_INDICES = [0, 1, 2, 3, 4, 5, 6, 7]
VALID_PHY_RATES = [
    6000000, 9000000, 12000000, 18000000, 24000000, 36000000, 48000000, 54000000  # 802.11a rates
]
VALID_CHANNEL_WIDTHS = [20, 40, 80, 160]

# FIXED: Issue #28 - Define constant/useless features to remove early
CONSTANT_USELESS_FEATURES = [
    'T1', 'T2', 'T3',
    'decisionReason',
    'offeredLoad',
    'queueLen',
    'retryCount'
]

# üîß FIXED: Issue C3 - Outcome features to remove (NEW!)
# These are results of the CURRENT rate choice, not inputs for rate selection
OUTCOME_FEATURES_TO_REMOVE = [
    'shortSuccRatio',   # Success rate of CURRENT rate (outcome)
    'medSuccRatio',     # Medium-term success rate (outcome)
    'packetLossRate',   # Loss rate of CURRENT rate (outcome)
    'severity',         # Derived from packetLossRate (outcome)
    'confidence',       # Derived from shortSuccRatio (outcome)
]

# Temporal leakage features (should already be removed by File 1b, but check)
TEMPORAL_LEAKAGE_FEATURES = [
    'consecSuccess',
    'consecFailure',
    'retrySuccessRatio',
    'timeSinceLastRateChange',
    'rateStabilityScore',
    'recentRateChanges',
    'packetSuccess'
]

# Known leaky features (should already be removed)
KNOWN_LEAKY_FEATURES = [
    'phyRate',
    'optimalRateDistance',
    'recentThroughputTrend',
    'conservativeFactor',
    'aggressiveFactor',
    'recommendedSafeRate'
]

# Data validation ranges (only for features that won't be removed)
VALIDATION_RANGES = {
    'lastSnr': (-10, 50),
    'snrFast': (-10, 50),
    'snrSlow': (-10, 50),
    'snrTrendShort': (-20, 20),
    'snrStabilityIndex': (0, 50),
    'snrPredictionConfidence': (0, 1),
    'channelWidth': (10, 160),
    'mobilityMetric': (0, 1000),
    'snrVariance': (0, 1000),
}

# CRITICAL: Class balancing DISABLED - happens in File 1b instead!
ENABLE_CLASS_BALANCING = False

# üîß FIXED: Issue C6 - Updated imbalance thresholds to match File 1b output
# File 1b with POWER=0.5 creates ~20x imbalance (down from ~100x natural)
IMBALANCE_THRESHOLDS = {
    'too_balanced': 10,      # <10x = suspiciously flat (artificial)
    'expected_min': 15,      # 15x = lower bound of File 1b output
    'expected_max': 30,      # 30x = upper bound of File 1b output
    'too_imbalanced': 50     # >50x = File 1b didn't work properly
}

# ================== SETUP ==================
def setup_logging():
    """Setup comprehensive logging"""
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)
    logger.info("="*80)
    logger.info("INTERMEDIATE ML DATA CLEANING - FULLY FIXED (REMOVES OUTCOME FEATURES)")
    logger.info("="*80)
    logger.info(f"Author: ahmedjk34")
    logger.info(f"Date: 2025-10-02 14:26:45 UTC")
    logger.info(f"Random Seed: {RANDOM_SEED} (Issue #14 - Reproducibility)")
    logger.info(f"Class Balancing: {'ENABLED' if ENABLE_CLASS_BALANCING else 'DISABLED (correct!)'}")
    logger.info("="*80)
    logger.info("FIXES APPLIED:")
    logger.info("  ‚úÖ Issue C6: Imbalance thresholds updated (15-30x expected)")
    logger.info("  ‚úÖ Issue M5: Validation expectations corrected")
    logger.info("  ‚úÖ Issue C3: OUTCOME FEATURES REMOVED (5 features)")
    logger.info("="*80)
    logger.info("")
    logger.info("‚ö†Ô∏è IMPORTANT: This file does NOT balance classes!")
    logger.info("   Class balancing happens in File 1b (CSV combiner) to maintain realistic distribution.")
    logger.info("   File 1b with POWER=0.5 creates ~20x imbalance (down from ~100x natural WiFi).")
    logger.info("   File 2 performs statistical cleaning AND removes outcome features.")
    logger.info("")
    logger.info("üîß OUTCOME FEATURES REMOVED:")
    logger.info(f"   {OUTCOME_FEATURES_TO_REMOVE}")
    logger.info("")
    return logger

def create_stats_directory():
    """Create directory for statistics outputs"""
    os.makedirs(STATS_DIR, exist_ok=True)
    return STATS_DIR

# ================== DATA LOADING ==================
def load_and_validate_data(filepath: str, logger) -> pd.DataFrame:
    """Load data with comprehensive validation"""
    logger.info(f"üìÇ Loading data from: {filepath}")
    
    if not os.path.exists(filepath):
        logger.error(f"‚ùå Input file does not exist: {filepath}")
        sys.exit(1)
    
    try:
        df = pd.read_csv(filepath, low_memory=False)
        logger.info(f"‚úÖ Successfully loaded {len(df)} rows, {len(df.columns)} columns")
        logger.info(f"üìä Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        return df
    except Exception as e:
        logger.error(f"‚ùå Failed to load data: {str(e)}")
        sys.exit(1)

# ================== FEATURE REMOVAL ==================
def remove_all_unwanted_features(df: pd.DataFrame, logger) -> pd.DataFrame:
    """
    üîß FIXED: Issue C3, #28 - Remove constant/useless/temporal/leaky/outcome features
    """
    logger.info("üßπ REMOVING ALL UNWANTED FEATURES (constant, temporal, leaky, outcome)...")
    
    # Combine all features to remove
    ALL_FEATURES_TO_REMOVE = list(set(
        CONSTANT_USELESS_FEATURES +
        TEMPORAL_LEAKAGE_FEATURES +
        KNOWN_LEAKY_FEATURES +
        OUTCOME_FEATURES_TO_REMOVE  # ‚Üê NEW! Removes outcome features
    ))
    
    initial_cols = len(df.columns)
    removed_features = []
    
    for feature in ALL_FEATURES_TO_REMOVE:
        if feature in df.columns:
            removed_features.append(feature)
    
    if removed_features:
        df_clean = df.drop(columns=removed_features)
        
        # Categorize removed features for logging
        removed_constant = [f for f in CONSTANT_USELESS_FEATURES if f in removed_features]
        removed_temporal = [f for f in TEMPORAL_LEAKAGE_FEATURES if f in removed_features]
        removed_leaky = [f for f in KNOWN_LEAKY_FEATURES if f in removed_features]
        removed_outcome = [f for f in OUTCOME_FEATURES_TO_REMOVE if f in removed_features]
        
        logger.info(f"üßπ Removed {len(removed_features)} unwanted features:")
        if removed_constant:
            logger.info(f"   ‚ùå Constant/useless ({len(removed_constant)}): {removed_constant}")
        if removed_temporal:
            logger.info(f"   ‚ùå Temporal leakage ({len(removed_temporal)}): {removed_temporal}")
        if removed_leaky:
            logger.info(f"   ‚ùå Known leaky ({len(removed_leaky)}): {removed_leaky}")
        if removed_outcome:
            logger.info(f"   ‚ùå Outcome features ({len(removed_outcome)}): {removed_outcome}")
        
        logger.info(f"üìä Dataset shape: {initial_cols} ‚Üí {len(df_clean.columns)} columns")
        
        print(f"\nüßπ REMOVED {len(removed_features)} UNWANTED FEATURES:")
        print(f"   Constant: {len(removed_constant)}")
        print(f"   Temporal: {len(removed_temporal)}")
        print(f"   Leaky: {len(removed_leaky)}")
        print(f"   Outcome: {len(removed_outcome)}")
        print(f"üìä Dataset now has {len(df_clean.columns)} columns (was {initial_cols})")
        
        return df_clean
    else:
        logger.info("‚úÖ No unwanted features found to remove")
        return df

# ================== COMPREHENSIVE STATISTICS ==================
def generate_comprehensive_statistics(df: pd.DataFrame, stage: str, logger) -> Dict[str, Any]:
    """Generate extensive statistical analysis"""
    logger.info(f"üìà Generating comprehensive statistics for {stage} stage...")
    
    stats_dict = {
        'stage': stage,
        'timestamp': datetime.now().isoformat(),
        'random_seed': RANDOM_SEED,
        'basic_info': {},
        'column_info': {},
        'numerical_stats': {},
        'missing_data': {},
        'wifi_specific': {}
    }
    
    # Basic information
    stats_dict['basic_info'] = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'duplicate_rows': df.duplicated().sum(),
    }
    
    # Column information
    stats_dict['column_info'] = {
        'column_names': list(df.columns),
        'data_types': df.dtypes.astype(str).to_dict(),
        'non_null_counts': df.count().to_dict()
    }
    
    # Numerical statistics
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if numerical_cols:
        numerical_stats = df[numerical_cols].describe(include='all')
        stats_dict['numerical_stats'] = numerical_stats.to_dict()
    
    # Missing data analysis
    missing_data = df.isnull().sum()
    missing_percentages = (missing_data / len(df) * 100).round(2)
    stats_dict['missing_data'] = {
        'missing_counts': missing_data.to_dict(),
        'missing_percentages': missing_percentages.to_dict(),
        'total_missing_values': int(missing_data.sum())
    }
    
    # WiFi-specific statistics
    wifi_stats = {}
    
    if 'rateIdx' in df.columns:
        rate_dist = df['rateIdx'].value_counts().sort_index()
        wifi_stats['rate_index_distribution'] = rate_dist.to_dict()
        
        # üîß FIXED: Issue C6 - Check if distribution matches File 1b expectations
        if len(rate_dist) >= 8:
            rate_imbalance = rate_dist.max() / rate_dist.min()
            wifi_stats['rate_imbalance_ratio'] = float(rate_imbalance)
            
            # Updated validation logic
            if rate_imbalance < IMBALANCE_THRESHOLDS['too_balanced']:
                logger.warning(f"‚ö†Ô∏è WARNING: Rate imbalance is {rate_imbalance:.1f}x (too balanced!)")
                logger.warning(f"   Expected: {IMBALANCE_THRESHOLDS['expected_min']}-{IMBALANCE_THRESHOLDS['expected_max']}x from File 1b")
                logger.warning(f"   This suggests File 1b balancing didn't work or wrong POWER setting")
            elif IMBALANCE_THRESHOLDS['expected_min'] <= rate_imbalance <= IMBALANCE_THRESHOLDS['expected_max']:
                logger.info(f"‚úÖ Rate imbalance is {rate_imbalance:.1f}x (matches File 1b POWER=0.5 output)")
                logger.info(f"   This is CORRECT behavior (File 1b balanced from ~100x to ~20x)")
            elif rate_imbalance > IMBALANCE_THRESHOLDS['too_imbalanced']:
                logger.warning(f"‚ö†Ô∏è WARNING: Rate imbalance is {rate_imbalance:.1f}x (too imbalanced!)")
                logger.warning(f"   Expected: {IMBALANCE_THRESHOLDS['expected_min']}-{IMBALANCE_THRESHOLDS['expected_max']}x from File 1b")
                logger.warning(f"   This suggests File 1b balancing failed or POWER too high")
            else:
                logger.info(f"‚úÖ Rate imbalance is {rate_imbalance:.1f}x (acceptable)")
    
    if 'lastSnr' in df.columns:
        snr_data = df['lastSnr'].dropna()
        wifi_stats['snr_analysis'] = {
            'mean': float(snr_data.mean()),
            'median': float(snr_data.median()),
            'std': float(snr_data.std()),
            'min': float(snr_data.min()),
            'max': float(snr_data.max()),
        }
    
    if 'scenario_file' in df.columns:
        scenario_dist = df['scenario_file'].value_counts()
        wifi_stats['scenario_distribution'] = {
            'total_scenarios': int(df['scenario_file'].nunique()),
            'top_10_scenarios': scenario_dist.head(10).to_dict(),
        }
    
    stats_dict['wifi_specific'] = wifi_stats
    
    return stats_dict

def save_statistics(stats_dict: Dict[str, Any], stage: str, stats_dir: str, logger):
    """Save statistics to JSON format"""
    logger.info(f"üíæ Saving statistics for {stage} stage...")
    
    def convert_numpy(obj):
        """Convert numpy types to Python types"""
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, dict):
            return {convert_numpy(k): convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_numpy(item) for item in obj]
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    stats_dict_converted = convert_numpy(stats_dict)
    
    json_file = os.path.join(stats_dir, f"statistics_{stage}.json")
    try:
        with open(json_file, 'w') as f:
            json.dump(stats_dict_converted, f, indent=2)
        logger.info(f"‚úÖ Statistics saved to {json_file}")
    except Exception as e:
        logger.error(f"‚ùå Failed to save statistics: {str(e)}")

def generate_visualizations(df: pd.DataFrame, stage: str, stats_dir: str, logger):
    """Generate key visualizations"""
    logger.info(f"üìä Generating visualizations for {stage} stage...")
    
    plt.style.use('default')
    
    # 1. Rate distribution
    if 'rateIdx' in df.columns:
        plt.figure(figsize=(12, 6))
        rate_counts = df['rateIdx'].value_counts().sort_index()
        
        plt.subplot(1, 2, 1)
        plt.bar(rate_counts.index, rate_counts.values, color='steelblue')
        plt.title(f'Rate Index Distribution - {stage}')
        plt.xlabel('Rate Index')
        plt.ylabel('Count')
        plt.grid(True, alpha=0.3)
        
        # Add imbalance ratio annotation
        if len(rate_counts) >= 8:
            imbalance = rate_counts.max() / rate_counts.min()
            color = 'green' if IMBALANCE_THRESHOLDS['expected_min'] <= imbalance <= IMBALANCE_THRESHOLDS['expected_max'] else 'red'
            plt.text(0.5, 0.95, f'Imbalance Ratio: {imbalance:.1f}x\n(Expected: {IMBALANCE_THRESHOLDS["expected_min"]}-{IMBALANCE_THRESHOLDS["expected_max"]}x)', 
                    transform=plt.gca().transAxes, ha='center',
                    bbox=dict(boxstyle='round', facecolor=color, alpha=0.3))
        
        plt.subplot(1, 2, 2)
        percentages = (rate_counts / rate_counts.sum() * 100).round(1)
        plt.bar(percentages.index, percentages.values, color='coral')
        plt.title(f'Rate Distribution (Percentage) - {stage}')
        plt.xlabel('Rate Index')
        plt.ylabel('Percentage (%)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(stats_dir, f'rate_distribution_{stage}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    # 2. SNR distribution
    if 'lastSnr' in df.columns:
        plt.figure(figsize=(10, 6))
        df['lastSnr'].hist(bins=50, alpha=0.7, color='green')
        plt.title(f'SNR Distribution - {stage}')
        plt.xlabel('SNR (dB)')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(stats_dir, f'snr_distribution_{stage}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    logger.info(f"‚úÖ Visualizations saved to {stats_dir}")

# ================== DATA CLEANING FUNCTIONS ==================
def remove_duplicates(df: pd.DataFrame, logger) -> pd.DataFrame:
    """Remove duplicate rows"""
    logger.info("üîç Checking for duplicate rows...")
    
    initial_count = len(df)
    duplicate_count = df.duplicated().sum()
    
    if duplicate_count > 0:
        logger.info(f"Found {duplicate_count} duplicate rows ({duplicate_count/initial_count*100:.2f}%)")
        df_clean = df.drop_duplicates()
        logger.info(f"‚úÖ Removed {duplicate_count} duplicate rows")
        return df_clean
    else:
        logger.info("‚úÖ No duplicate rows found")
        return df

def enforce_data_types(df: pd.DataFrame, logger) -> pd.DataFrame:
    """Enforce proper data types"""
    logger.info("üîß Enforcing data types...")
    
    df_clean = df.copy()
    type_conversions = 0
    
    expected_types = {
        'time': 'float64',
        'stationId': 'int64',
        'rateIdx': 'int64',
        'lastSnr': 'float64',
        'snrFast': 'float64',
        'snrSlow': 'float64',
        'channelWidth': 'int64',
    }
    
    for col, expected_type in expected_types.items():
        if col in df_clean.columns:
            try:
                if df_clean[col].dtype != expected_type:
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce', downcast=None)
                    if expected_type.startswith('int'):
                        df_clean[col] = df_clean[col].round().astype('Int64')
                    type_conversions += 1
            except Exception:
                pass
    
    logger.info(f"‚úÖ Completed {type_conversions} type conversions")
    return df_clean

def validate_wifi_constraints(df: pd.DataFrame, logger) -> pd.DataFrame:
    """Validate WiFi-specific constraints"""
    logger.info("üì° Validating WiFi constraints...")
    
    df_clean = df.copy()
    initial_count = len(df_clean)
    
    if 'rateIdx' in df_clean.columns:
        valid_mask = df_clean['rateIdx'].notna() & df_clean['rateIdx'].isin(VALID_RATE_INDICES)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            logger.info(f"  ‚ö†Ô∏è Removing {invalid_count} rows with invalid rate indices")
            df_clean = df_clean[valid_mask]
    
    if 'channelWidth' in df_clean.columns:
        valid_mask = df_clean['channelWidth'].notna() & df_clean['channelWidth'].isin(VALID_CHANNEL_WIDTHS)
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            logger.info(f"  ‚ö†Ô∏è Removing {invalid_count} rows with invalid channel widths")
            df_clean = df_clean[valid_mask]
    
    removed_count = initial_count - len(df_clean)
    logger.info(f"‚úÖ WiFi validation complete. Removed {removed_count} invalid rows")
    return df_clean

def filter_outliers(df: pd.DataFrame, logger) -> pd.DataFrame:
    """Filter outliers based on validation ranges"""
    logger.info("üéØ Filtering outliers based on validation ranges...")
    
    df_clean = df.copy()
    initial_count = len(df_clean)
    
    for col, (min_val, max_val) in VALIDATION_RANGES.items():
        if col in df_clean.columns:
            outliers = (df_clean[col] < min_val) | (df_clean[col] > max_val)
            outlier_count = outliers.sum()
            
            if outlier_count > 0:
                logger.info(f"  ‚ö†Ô∏è {col}: Removing {outlier_count} outliers "
                          f"(outside [{min_val}, {max_val}])")
                df_clean = df_clean[~outliers]
    
    total_removed = initial_count - len(df_clean)
    logger.info(f"‚úÖ Outlier filtering complete. Removed {total_removed} rows "
               f"({total_removed/initial_count*100:.2f}%)")
    return df_clean

def handle_missing_data(df: pd.DataFrame, logger) -> pd.DataFrame:
    """Handle missing data"""
    logger.info("üîß Handling missing data...")
    
    df_clean = df.copy()
    initial_count = len(df_clean)
    
    # Only check critical columns that still exist after feature removal
    critical_columns = ['rateIdx', 'lastSnr']
    existing_critical = [c for c in critical_columns if c in df_clean.columns]
    
    if existing_critical:
        missing_critical = df_clean[existing_critical].isnull().any(axis=1)
        critical_missing_count = missing_critical.sum()
        
        if critical_missing_count > 0:
            logger.info(f"  ‚ùå Removing {critical_missing_count} rows missing critical columns")
            df_clean = df_clean[~missing_critical]
    
    logger.info(f"‚úÖ Missing data handling complete. Retained {len(df_clean)}/{initial_count} rows")
    return df_clean

# ================== MAIN PIPELINE ==================
def main():
    """Main pipeline execution - Statistical cleaning + outcome feature removal"""
    logger = setup_logging()
    stats_dir = create_stats_directory()
    
    try:
        # Load data
        df_raw = load_and_validate_data(INPUT_CSV, logger)
        
        # Remove ALL unwanted features (constant, temporal, leaky, outcome)
        df_raw = remove_all_unwanted_features(df_raw, logger)
        
        # Generate initial statistics
        initial_stats = generate_comprehensive_statistics(df_raw, "initial", logger)
        save_statistics(initial_stats, "initial", stats_dir, logger)
        generate_visualizations(df_raw, "initial", stats_dir, logger)
        
        # Start cleaning pipeline
        logger.info("\n" + "="*60)
        logger.info("STARTING DATA CLEANING PIPELINE")
        logger.info("="*60)
        
        df_clean = remove_duplicates(df_raw, logger)
        df_clean = enforce_data_types(df_clean, logger)
        df_clean = validate_wifi_constraints(df_clean, logger)
        df_clean = filter_outliers(df_clean, logger)
        df_clean = handle_missing_data(df_clean, logger)
        
        # Generate final statistics
        final_stats = generate_comprehensive_statistics(df_clean, "final", logger)
        save_statistics(final_stats, "final", stats_dir, logger)
        generate_visualizations(df_clean, "final", stats_dir, logger)
        
        # Check final distribution
        if 'rateIdx' in df_clean.columns:
            rate_dist = df_clean['rateIdx'].value_counts().sort_index()
            logger.info("\nüìä Final rate distribution:")
            for rate, count in rate_dist.items():
                pct = (count / len(df_clean)) * 100
                logger.info(f"   Rate {rate}: {count:,} ({pct:.1f}%)")
            
            imbalance = rate_dist.max() / rate_dist.min()
            logger.info(f"\nüìä Rate imbalance ratio: {imbalance:.1f}x")
            
            # üîß FIXED: Issue C6 - Corrected validation logic
            if imbalance < IMBALANCE_THRESHOLDS['too_balanced']:
                logger.warning(f"‚ö†Ô∏è WARNING: Distribution is {imbalance:.1f}x (too balanced!)")
                logger.warning(f"   Expected: {IMBALANCE_THRESHOLDS['expected_min']}-{IMBALANCE_THRESHOLDS['expected_max']}x from File 1b")
            elif IMBALANCE_THRESHOLDS['expected_min'] <= imbalance <= IMBALANCE_THRESHOLDS['expected_max']:
                logger.info(f"‚úÖ Distribution looks correct ({imbalance:.1f}x matches File 1b output)")
            elif imbalance > IMBALANCE_THRESHOLDS['too_imbalanced']:
                logger.warning(f"‚ö†Ô∏è WARNING: Distribution is {imbalance:.1f}x (too imbalanced!)")
                logger.warning(f"   Expected: {IMBALANCE_THRESHOLDS['expected_min']}-{IMBALANCE_THRESHOLDS['expected_max']}x from File 1b")
        
        # Save cleaned data
        logger.info(f"\nüíæ Saving cleaned data to: {OUTPUT_CSV}")
        df_clean.to_csv(OUTPUT_CSV, index=False)
        
        # Final summary
        logger.info("\n" + "="*60)
        logger.info("CLEANING PIPELINE COMPLETE (FULLY FIXED)")
        logger.info("="*60)
        logger.info(f"üìä Initial rows: {len(df_raw):,}")
        logger.info(f"üìä Final rows: {len(df_clean):,}")
        logger.info(f"üìä Rows removed: {len(df_raw) - len(df_clean):,} "
                   f"({(len(df_raw) - len(df_clean))/len(df_raw)*100:.1f}%)")
        logger.info(f"üìÅ Output file: {OUTPUT_CSV}")
        logger.info(f"üîß Outcome features removed: {OUTCOME_FEATURES_TO_REMOVE}")
        logger.info(f"‚úÖ Dataset ready for File 3 (oracle label generation)")
        
        print(f"\n‚úÖ CLEANING COMPLETE (FULLY FIXED)!")
        print(f"üìä {len(df_raw):,} ‚Üí {len(df_clean):,} rows")
        print(f"üìÅ Cleaned data: {OUTPUT_CSV}")
        print(f"üîß Removed {len(OUTCOME_FEATURES_TO_REMOVE)} outcome features")
        print(f"‚úÖ Ready for File 3!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Pipeline failed: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)